{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3356f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa282cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_bert = pipeline('sentiment-analysis')\n",
    "classifier_finbert = pipeline('sentiment-analysis', model=\"ProsusAI/finbert\")\n",
    "classifier_roberta = pipeline('sentiment-analysis', model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "classifier_roberta2 = pipeline('sentiment-analysis', model=\"textattack/roberta-base-SST-2\")\n",
    "classifier_roberta_imdb = pipeline('sentiment-analysis', model=\"textattack/roberta-base-imdb\")\n",
    "classifier_roberta_imdb2 = pipeline('sentiment-analysis', model=\"aychang/roberta-base-imdb\")\n",
    "classifier_xlnet = pipeline('sentiment-analysis', model=\"edwardgowsmith/xlnet-base-cased-best\")\n",
    "classifier_xlnet2 = pipeline('sentiment-analysis', model=\"textattack/xlnet-base-cased-SST-2\")\n",
    "classifier_electra = pipeline('sentiment-analysis', model=\"howey/electra-large-sst2\")\n",
    "\n",
    "us_fed_chairs = [\"Jerome H Powell\", \"Janet L Yellen\", \"Ben S Bernanke\", \"Alan Greenspan\"]\n",
    "us_board_speakers = [\"Alice M Rivlin\", \"Daniel K Tarullo\", \"Edward M Gramlich\", \n",
    "                         \"Edward W Kelley Jr\", \"Elizabeth A Duke\", \"Frederic S Mishkin\",\n",
    "                        \"Jeremy C Stein\", \"Kevin M Warsh\", \"Lael Brainard\",\n",
    "                        \"Laurence H Meyer\", \"Mark W Olson\", \"Randall S Kroszner\",\n",
    "                        \"Robert W Ferguson Jr\", \"Sarah Bloom Raskin\", \"Susan Schmidt Bies\"]\n",
    "us_president = [\"Thomas M Hoenig\", \"Timothy F Geithner\", \"William C Dudley\", \"William J McDonough\", \"Brian P Sack\",\n",
    "               \"Charles I Plosser\", \"Ernest T Patrikis\", \"James McAndrews\", \"Joseph S Tracy\", \"Narayana Kocherlakota\", \n",
    "                \"Simon M Potter\" ]\n",
    "us_speekers_to_keep = us_fed_chairs + us_board_speakers + us_president\n",
    "\n",
    "df = pd.read_csv(\"source/speech_index_manual.csv\")\n",
    "df = df.loc[df[\"author\"].isin(us_speekers_to_keep)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758460f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_intro_length = 315\n",
    "t_init = time.time()\n",
    "count = 0\n",
    "\n",
    "def del_content(i, i_init, text):\n",
    "    # recursively returns list of all indexes in the speech file that should be deleted.\n",
    "    # tokens to be deleted are included in the two lsits below\n",
    "    del_words = [\"bis\", \"review\"] + [str(i) for i in range(0,10)]\n",
    "    del_end = [\"/2000\"]\n",
    "    list_to_delete = [] #list of indexes to be returned and deleted\n",
    "    if text[i].lower() in del_words:  #if word matches words in the delete list\n",
    "        list_to_delete = list_to_delete + [i]\n",
    "    elif text[i][-5:] in del_end:  #if end of word matches end of words in the delete list\n",
    "        list_to_delete  = list_to_delete = [i]\n",
    "    else:  # not a word we want to delete, return empty list\n",
    "        return []  \n",
    "    if i <= i_init:  # continue checking leftward\n",
    "        list_to_delete = list_to_delete + del_content(i - 1, i_init, text)\n",
    "    if i >= i_init: #continue checking rightward\n",
    "        try:  # TODO - try/except for the special case of the deleted item being the last item in the text\n",
    "            list_to_delete = list_to_delete + del_content(i + 1, i_init, text)\n",
    "        except:\n",
    "            pass\n",
    "    return list_to_delete\n",
    "\n",
    "def filter_content(text):\n",
    "    star_counter = 0\n",
    "    for i, letter in enumerate(text):\n",
    "        if letter == \"*\":\n",
    "            star_counter += 1\n",
    "        if star_counter >= 3:\n",
    "            text = text[i + 1:len(text)]\n",
    "            break\n",
    "    split_text = text.split()\n",
    "    list_to_delete = []\n",
    "    for i, word in enumerate(split_text):\n",
    "        if word.lower() == \"bis\":\n",
    "            list_to_delete = list_to_delete + del_content(i, i, split_text)\n",
    "    # have to be careful -- deleting items of list I'm iterating over\n",
    "    # delete indexes in descending order to avoid index out of range\n",
    "    list_to_delete.sort()\n",
    "    for item_to_delete in reversed(list_to_delete):\n",
    "        split_text.pop(item_to_delete)\n",
    "        \n",
    "    # TEMP TODO - only taking 300 first words because of tokenization limit\n",
    "    split_text = split_text[0:max_word_intro_length]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def sentiment_analysis(speech_file):\n",
    "    with open(\"source/txt/\" + speech_file, \"r\", encoding=\"utf8\") as content:\n",
    "        filtered_content = filter_content(content.read())\n",
    "        bert_sent = classifier_bert(filtered_content)\n",
    "        finbert_sent = classifier_finbert(filtered_content)\n",
    "        robertat_sent = classifier_roberta(filtered_content)\n",
    "        robertat_sent2 = classifier_roberta2(filtered_content)\n",
    "        robertat_imdb_sent = classifier_roberta_imdb(filtered_content)\n",
    "        robertat_imdb_sent2 = classifier_roberta_imdb2(filtered_content)\n",
    "        xlnet_sent = classifier_xlnet(filtered_content)\n",
    "        xlnet_sent2 = classifier_xlnet2(filtered_content)\n",
    "        electra_sent = classifier_electra(filtered_content)\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"bert_sent\"]] = 2 if bert_sent[0][\"label\"] == \"POSITIVE\" else 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"bert_score\"]] = bert_sent[0][\"score\"]\n",
    "        if robertat_sent[0][\"label\"] == \"LABEL_2\":\n",
    "            df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_sent\"]] = 2\n",
    "        elif robertat_sent[0][\"label\"] == \"LABEL_1\":\n",
    "            df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_sent\"]] = 1\n",
    "        else:\n",
    "            df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_sent\"]] = 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_score\"]] = robertat_sent[0][\"score\"]\n",
    "\n",
    "        if finbert_sent[0][\"label\"] == \"positive\":\n",
    "            df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"finbert_sent\"]] = 2\n",
    "        elif finbert_sent[0][\"label\"] == \"neutral\":\n",
    "            df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"finbert_sent\"]] = 1\n",
    "        else:\n",
    "            df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"finbert_sent\"]] = 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"finbert_score\"]] = finbert_sent[0][\"score\"]\n",
    "\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_sent2\"]] = 2 if robertat_sent2[0][\"label\"] == \"LABEL_1\" else 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_score2\"]] = robertat_sent2[0][\"score\"]\n",
    "\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_imdb_sent\"]] = 2 if robertat_imdb_sent[0][\"label\"] == \"LABEL_1\" else 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_imdb_score\"]] = robertat_imdb_sent[0][\"score\"]\n",
    "\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_imdb_sent2\"]] = 2 if robertat_imdb_sent2[0][\"label\"] == \"pos\" else 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"roberta_imdb_score2\"]] = robertat_imdb_sent2[0][\"score\"]\n",
    "\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"xlnet_sent\"]] = 2 if xlnet_sent[0][\"label\"] == \"LABEL_1\" else 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"xlnet_score\"]] = xlnet_sent[0][\"score\"]\n",
    "\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"xlnet_sent2\"]] = 2 if xlnet_sent2[0][\"label\"] == \"LABEL_1\" else 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"xlnet_score2\"]] = xlnet_sent2[0][\"score\"]\n",
    "\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"electra_sent\"]] = 2 if electra_sent[0][\"label\"] == \"LABEL_1\" else 0\n",
    "        df.loc[df[\"speech\"] == speech_file.split(\".\")[0], [\"electra_score\"]] = electra_sent[0][\"score\"]\n",
    "\n",
    "for root, dirst, files in os.walk(\"source/txt\"):\n",
    "    for i, speech_file in enumerate(files):\n",
    "        if speech_file.split(\".\")[0] in df.speech.values:\n",
    "            count += 1\n",
    "            if count % 25 == 0:\n",
    "                print(\"count: \" + str(count) + \" - time: \" + str(time.time() - t_init))\n",
    "            sentiment_analysis(speech_file)\n",
    "            \n",
    "            \n",
    "                \n",
    "df.to_csv(\"data_files_generated/genered_speech_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_csv(\"source/usnews.csv\")\n",
    "df_news.head()\n",
    "print(df_news.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac9a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_content_news(text):\n",
    "    split_text = text.split()\n",
    "    split_text = split_text[0:300]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "t_init = time.time()\n",
    "count = 0\n",
    "for i, row in df_news.iterrows():\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(\"count: \" + str(count) + \" - time: \" + str(time.time() - t_init))\n",
    "    #print(row[\"texts\"])\n",
    "    filtered_content = filter_content_news(row[\"texts\"])\n",
    "    bert_sent = classifier_bert(filtered_content)\n",
    "    robertat_sent = classifier_roberta(filtered_content)\n",
    "    xlnet_sent = classifier_xlnet(filtered_content)\n",
    "    electra_sent = classifier_electra(filtered_content)\n",
    "    df_news.loc[i, [\"news_bert_sent\"]] = 2 if bert_sent[0][\"label\"] == \"POSITIVE\" else 0\n",
    "    df_news.loc[i, [\"news_bert_score\"]] = bert_sent[0][\"score\"]\n",
    "    if robertat_sent[0][\"label\"] == \"LABEL_2\":\n",
    "        df_news.loc[i, [\"news_roberta_sent\"]] = 2\n",
    "    elif robertat_sent[0][\"label\"] == \"LABEL_1\":\n",
    "        df_news.loc[i, [\"news_roberta_sent\"]] = 1\n",
    "    else:\n",
    "        df_news.loc[i, [\"news_roberta_sent\"]] = 0\n",
    "    df_news.loc[i, [\"news_roberta_score\"]] = robertat_sent[0][\"score\"]\n",
    "    df_news.loc[i, [\"news_xlnet_sent\"]] = 2 if xlnet_sent[0][\"label\"] == \"LABEL_1\" else 0\n",
    "    df_news.loc[i, [\"news_xlnet_score\"]] = xlnet_sent[0][\"score\"]\n",
    "    df_news.loc[i, [\"news_electra_sent\"]] = 2 if electra_sent[0][\"label\"] == \"LABEL_1\" else 0\n",
    "    df_news.loc[i, [\"news_electra_score\"]] = electra_sent[0][\"score\"]\n",
    "    \n",
    "df_news.to_csv(\"data_files_generated/generated_news_sentiment_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33865a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_generated = pd.DataFrame({'date':pd.date_range('01/01/1995', periods=10000)})\n",
    "df_news_generated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prob(sent, score):\n",
    "    return 1\n",
    "\n",
    "#print(df_news[\"date\"])\n",
    "sent_list = [\"news_bert_sent\", \"news_roberta_sent\", \"news_xlnet_sent\", \"news_electra_sent\"]\n",
    "score_list = [\"news_bert_score\", \"news_roberta_score\", \"news_xlnet_score\", \"news_electra_score\"]\n",
    "weighted_score_list = [0 for i in range(len(sent_list))]\n",
    "for i, row in df_news_generated.iterrows():\n",
    "    #print(row[\"date\"].date())\n",
    "    #if str(row[\"date\"].date()) in df_news.date.values:\n",
    "    if str(row[\"date\"].date()) in df_news[\"date\"].values:\n",
    "        #print(df_news[df_news[\"date\"] == str(row[\"date\"].date())].news_bert_score.values)\n",
    "        articles = df_news[df_news[\"date\"] == str(row[\"date\"].date())]\n",
    "        count_article = len(articles)\n",
    "        wsj = 0\n",
    "        wapo = 0\n",
    "        econ = 0\n",
    "        nonecon = 0\n",
    "        bert_positive = 0\n",
    "        bert_negative = 0\n",
    "        xlnet_positive = 0\n",
    "        xlnet_negative = 0\n",
    "        electra_positive = 0\n",
    "        electra_negative = 0\n",
    "        for j in range(count_article):\n",
    "            bert_positive += articles.news_bert_sent.values[j] / 2\n",
    "            bert_negative += 1 - articles.news_bert_sent.values[j] / 2\n",
    "            xlnet_positive += articles.news_xlnet_sent.values[j] / 2\n",
    "            xlnet_negative += 1 - articles.news_xlnet_sent.values[j] / 2\n",
    "            electra_positive += articles.news_bert_sent.values[j] / 2\n",
    "            electra_negative += 1 - articles.news_electra_sent.values[j] / 2\n",
    "            wsj += articles.wsj.values[j]\n",
    "            wapo += articles.wapo.values[j]\n",
    "            econ += articles.economy.values[j]\n",
    "            nonecon += articles.noneconomy.values[j]\n",
    "        \n",
    "        df_news_generated.loc[i, \"bert_positive\"] = bert_positive\n",
    "        df_news_generated.loc[i, \"bert_negative\"] = bert_negative\n",
    "        df_news_generated.loc[i, \"xlnet_positive\"] = xlnet_positive\n",
    "        df_news_generated.loc[i, \"xlnet_negative\"] = xlnet_negative\n",
    "        df_news_generated.loc[i, \"electra_positive\"] = electra_positive\n",
    "        df_news_generated.loc[i, \"electra_negative\"] = electra_negative\n",
    "        \n",
    "        df_news_generated.loc[i, \"wsj\"] = wsj\n",
    "        df_news_generated.loc[i, \"wapo\"] = wapo\n",
    "        df_news_generated.loc[i, \"econ\"] = econ\n",
    "        df_news_generated.loc[i, \"nonecon\"] = nonecon\n",
    "\n",
    "df_news_generated.to_csv(\"data_files_generated/generated_news_sentiment_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
